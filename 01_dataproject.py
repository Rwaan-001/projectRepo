# -*- coding: utf-8 -*-
"""01_dataPROJECT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1o7UxXiSPJfFCwn-jNV8XehSFfwivWIAE
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import pearsonr, spearmanr, chi2_contingency, ttest_1samp, skew, kurtosis

import pandas as pd

DATA_PATH = "/content/shopping_behavior_updatedNEW.csv"
df = pd.read_csv(DATA_PATH)

TARGET = "Purchase Amount (USD)"
NUM_IV = "Age"                 # numerical IV for Pearson/Spearman
CAT_IV = "Discount Applied"    # categorical IV for Chi-square

df = df.copy()
if df[TARGET].isna().any():
    df = df.dropna(subset=[TARGET])  # drop rows missing the target

display(df.head())

"""# **TASK** 2: Q2 Create a function for Descriptive Statistics"""

print("Unique values in 'Category' column:")
print(df['Category'].unique())

#(TASK2) Q2:

# Function for descriptive statistics
def descriptive_stats(data, field):
    """Function to calculate descriptive statistics for a given column."""
    s = pd.to_numeric(data[field], errors='coerce').dropna()

    stats = {
        "Count": s.count(),
        "Mean": s.mean(),
        "Median": s.median(),
        "Mode": s.mode().iloc[0] if not s.mode().empty else np.nan,
        "Minimum": s.min(),
        "Maximum": s.max(),
        "Range": s.max() - s.min(),
        "Standard Deviation": s.std(),
        "Variance": s.var(),
        "Skewness": s.skew(),
        "Kurtosis": s.kurt(),
        "25th Percentile": s.quantile(0.25),
        "50th Percentile": s.quantile(0.50),
        "75th Percentile": s.quantile(0.75),
        "Interquartile Range": s.quantile(0.75) - s.quantile(0.25)
    }

    return stats

# Example: apply the function to the dependent variable
results = descriptive_stats(df, "Purchase Amount (USD)")
print("Descriptive Statistics for 'Purchase Amount (USD)':\n")
for k, v in results.items():
    print(f"{k}: {v:.2f}" if isinstance(v, (int, float, np.floating)) else f"{k}: {v}")

"""# **TASK 2: Q3**"""

# (TASK 2) Q3: sample 150 rows and apply the function from Q2
import pandas as pd

from pathlib import Path
csv_path = Path("/content/shopping_behavior_updatedNEW.csv") # used the original file that has big data since question asked for 150 as sample size

# 1) Load data
df = pd.read_csv(csv_path)

# 2) Choose dependent variable (change if needed)
dep_var = "Purchase Amount (USD)"

# 3) Take a random sample of size 150 (reproducible)
# Check if sample size is larger than the population
sample_size = 150
if sample_size > len(df):
    print(f"Warning: Sample size ({sample_size}) is larger than the total number of rows ({len(df)}). Sampling the entire dataset.")
    sample_size = len(df)

sample_150 = df.sample(n=sample_size, replace=False, random_state=42)


print(f"Original rows: {len(df)}")
print(f"Sample rows:   {len(sample_150)}")

# 4) Apply the Task-2 function to the SAMPLE (not the full df)
from math import isnan  # optional, if you want to check mode
# Re-define the descriptive_stats function if it's not available in the current session
def descriptive_stats(data, field):
    """Function to calculate descriptive statistics for a given column."""
    s = pd.to_numeric(data[field], errors='coerce').dropna()

    stats = {
        "Count": s.count(),
        "Mean": s.mean(),
        "Median": s.median(),
        "Mode": s.mode().iloc[0] if not s.mode().empty else np.nan,
        "Minimum": s.min(),
        "Maximum": s.max(),
        "Range": s.max() - s.min(),
        "Standard Deviation": s.std(),
        "Variance": s.var(),
        "Skewness": s.skew(),
        "Kurtosis": s.kurt(),
        "25th Percentile": s.quantile(0.25),
        "50th Percentile": s.quantile(0.50),
        "75th Percentile": s.quantile(0.75),
        "Interquartile Range": s.quantile(0.75) - s.quantile(0.25)
    }

    return stats

stats_150 = descriptive_stats(sample_150, dep_var)

# 5) Show results
print(f"\nDescriptive statistics for SAMPLE (n={sample_size}) â€” '{dep_var}':")
for k, v in stats_150.items():
    print(f"{k}: {v}")

"""# **TASK 2: Q4**"""

def systematic_sampling(data, step, start=0):
    """
    Systematic sampling function
    Condition: Select every kth record starting from specified position
    """
    indices = np.arange(start, len(data), step)
    return data.iloc[indices]

# Apply systematic sampling with conditions
step = 15        # Condition 1: Select every 15th record
start_pos = 3    # Condition 2: Start from the 4th record (index 3)

systematic_sample = systematic_sampling(df, step, start_pos)

print("SYSTEMATIC SAMPLING WITH CONDITIONS")
print(f"Condition 1: Select every {step}th record")
print(f"Condition 2: Start from position {start_pos + 1} (index {start_pos})")
print(f"Original dataset size: {len(df)}")
print(f"Systematic sample size: {len(systematic_sample)}")

# Apply descriptive function to dependent variable
dependent_variable = 'Purchase Amount (USD)'
systematic_stats = descriptive_stats(systematic_sample, dependent_variable)

print(f"\nDESCRIPTIVE STATISTICS FOR DEPENDENT VARIABLE: {dependent_variable}")
# Iterate through the columns and print the single value in each column
for col in systematic_stats.columns:
    value = systematic_stats[col].iloc[0]
    if isinstance(value, (int, float, np.floating)):
         print(f"{col}: {value:.2f}")
    else:
        print(f"{col}: {value}")

"""(TASK 2) Q5: create a detailed descripitve statitsics report about the dependent variable of the chosen dataset.




"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Choose dependent variable (edit if yours is different)
dep_var = "Purchase Amount (USD)"

# Choose dependent variable (edit if yours is different)
dep_var = "Purchase Amount (USD)"

desc = df[dep_var].describe()
extra = {
    "variance": df[dep_var].var(ddof=1),
    "skewness": df[dep_var].skew(),
    "kurtosis": df[dep_var].kurt()
}
print("Descriptive Statistics for", dep_var)
print(desc)
print(extra)

"""# **(TASK 2) Q6: visulaize the dependent variable by the graph/chart of the following using python program:**

A. Scatter Plot

B. Box Plot

C. Histogram

D. Heat Map
"""

#A: Scatter Plot

# load the data
df = pd.read_csv('/content/shopping_behavior_100.csv') #data size = 30
# Define the dependent variable
dep_var = "Purchase Amount (USD)"
#Scatter plot:
plt.figure()
plt.scatter(df.index, df[dep_var])
plt.title(f"Scatter Plot of {dep_var}")
plt.xlabel("Index")
plt.ylabel(dep_var)
plt.tight_layout()
plt.show()

#B. Box Plot

# load the data
df = pd.read_csv('/content/shopping_behavior_100.csv') #data size = 1000

#Box plot
plt.figure()
plt.boxplot(df[dep_var].dropna(), vert=False)
plt.title(f"Box Plot of {dep_var}")
plt.xlabel(dep_var)
plt.tight_layout()
plt.show()

#C. Histogram

# load the data
df = pd.read_csv('/content/shopping_behavior_100.csv') #data size = 1000
#Histogram:
plt.figure()
plt.hist(df[dep_var].dropna(), bins=10, edgecolor='black')
plt.title(f"Histogram of {dep_var}")
plt.xlabel(dep_var)
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

#D. Heat Map:
num = df.select_dtypes(include=[np.number])
corr = num.corr(numeric_only=True)

plt.figure()
im = plt.imshow(corr, aspect='auto')
plt.title("Correlation Heat Map (Numeric Features)")
plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha='right')
plt.yticks(range(len(corr.index)), corr.index)
plt.colorbar(im)
plt.tight_layout()
plt.show()

"""# **TASK 2: Q7**"""

import pandas as pd

from scipy.stats import pearsonr, spearmanr, chi2_contingency

import numpy as np



# load the data

df = pd.read_csv('/content/shopping_behavior_updatedNEW.csv')



# - NUMERICAL TESTS:

# Age (independent) vs Purchase Amount (dependent)

x = df['Age']

y = df['Purchase Amount (USD)']



# Pearson test (linear relationship)

pearson_r, pearson_p = pearsonr(x, y)



# Spearman test (rank/monotonic relationship)

spearman_r, spearman_p = spearmanr(x, y)



print("Numerical Correlation Tests: ")

print(f"Pearson Correlation: {pearson_r:.2f}, p-value: {pearson_p:.3f}")

print(f"Spearman Correlation: {spearman_r:.2f}, p-value: {spearman_p:.3f}")



if pearson_p < 0.05:
    print("Pearson: There IS a significant relationship between Age and Purchase Amount.")
else:
    print("Pearson: There is NO significant relationship between Age and Purchase Amount.")



if spearman_p < 0.05:
    print("Spearman: There IS a significant relationship between Age and Purchase Amount.")
else:
    print("Spearman: There is NO significant relationship between Age and Purchase Amount.")



# - CATEGORICAL TEST:

# Gender (independent) vs Purchase Amount level (High/Low)

df['_PurchaseLevel'] = np.where(df['Purchase Amount (USD)'] >= df['Purchase Amount (USD)'].median(), 'High', 'Low')



# Create contingency table

table = pd.crosstab(df['Gender'], df['_PurchaseLevel'])



# Chi-square test

chi2, p_chi, dof, exp = chi2_contingency(table)



print()



print("Categorical Test (Chi-Square): ")

print(f"Chi-square value: {chi2:.2f} | p-value: {p_chi:.3f}")



if p_chi < 0.05:
    print("Chi-Square: There IS a significant relationship between Gender and Purchase Amount.")
else:
    print("Chi-Square: There is NO significant relationship between Gender and Purchase Amount.")

"""# **TASK2: Q8**"""

# Q8: One-Sample T-Test using population mean from full dataset

import pandas as pd
import numpy as np
from scipy import stats


df = pd.read_csv("shopping_behavior_updatedNEW.csv")

def systematic_sampling(data, step, start=0):
    indices = np.arange(start, len(data), step)
    return data.iloc[indices]

step = 15
start_pos = 3
systematic_sample = systematic_sampling(df, step, start_pos)

# Select dependent variable
dep_var = "Purchase Amount (USD)"

# Population mean from the full dataset
population_mean = df[dep_var].mean()

# Sample values from the systematic sample
sample_values = systematic_sample[dep_var].dropna()

# Perform one-sample t-test
t_stat, p_value = stats.ttest_1samp(sample_values, population_mean)

print("One-Sample T-Test Results")
print("--------------------------")
print(f"Population Mean: {population_mean:.2f}")
print(f"Sample Mean: {sample_values.mean():.2f}")
print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Interpretation
alpha = 0.05
if p_value < alpha:
    print("Conclusion: The sample mean is significantly different from the population mean.")
    print("The sample may not represent the normal population.")
else:
    print("Conclusion: There is no significant difference between the sample and population mean.")
    print("The sample represents the normal population.")

"""# **TASK 3: Q9**"""

# Define independent (X) and dependent (Y) variables
X = df.iloc[:,[15]].values         # independent variable (Previous Purchases)
y = df.iloc[:,5].values          # dependent variable (Purchase Amount)

# Split data into train and test sets (80% / 20%)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(X_train)
print(X_test)
print(y_train)
print(y_test)

# Training the Simple Linear Regression model on the Training set
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)

# Predicting the Test results
y_pred = regressor.predict(X_test)
print(X_test)
print(y_pred)

regressor.predict([[100]])

# Visualize the Training set results
plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color = 'blue')
plt.title('Purchase Amount vs Previous Purchases (Training set)')
plt.xlabel('Previous Purchases')
plt.ylabel('Purchase Amount (USD)')
plt.show()

# Visualizing the Test set results
plt.scatter(X_test, y_test, color = 'pink')
plt.plot(X_train, regressor.predict(X_train), color = 'purple')
plt.title('Purchase Amount vs Previous Purchases (Test set)')
plt.xlabel('Previous Purchases')
plt.ylabel('Purchase Amount (USD)')
plt.show()

# get the intercept and the coefficent
b0 = regressor.intercept_
b1 = regressor.coef_
print('The intercept is:', b0)
print('The coefficent is:', b1[0])

from sklearn.metrics import r2_score

# model evaluation
r2_score(y_test, y_pred)

"""# **TASK 3: Q12**"""

# Step 12: Evaluate Models using Confusion Matrix and Accuracy

# Evaluate models using confusion matrix & accuracy
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("shopping_behavior_updatedNEW.csv")

# 2. Encode categorical columns
le = LabelEncoder()
for col in df.select_dtypes(include="object").columns:
    df[col] = le.fit_transform(df[col])

# 3. Define features (X) and target (y)
# Change the target column name if different in your dataset
target_column = 'Purchase Amount (USD)' # Assuming 'Purchase Amount (USD)' is the target based on previous cells
X = df.drop(columns=[target_column])

# Convert 'Purchase Amount (USD)' into a binary target variable based on the median
median_purchase = df[target_column].median()
y = (df[target_column] >= median_purchase).astype(int) # 1 for high purchase, 0 for low purchase


# 4. Split into train/test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. Define models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "KNN": KNeighborsClassifier()
}

# 6. Evaluate each model
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"\n{name}")
    print("Accuracy:", round(acc, 4))
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm)
    disp.plot(cmap="Blues")
    plt.title(f"{name} - Confusion Matrix")
    plt.show()

# 7. Identify and display the best model
best_model = max(results, key=results.get)
print("\nBest Classifier:", best_model)
print("Accuracy:", round(results[best_model], 4))

"""# **TASK 3: Q13**"""

#Step 13: Predict using Best-Fit Model (Random Forest)

from sklearn.ensemble import RandomForestClassifier

best_model = RandomForestClassifier()
best_model.fit(X_train, y_train)

#Predict on test data
y_pred = best_model.predict(X_test)

#Display sample predictions
predictions = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})
print("\nSample Predictions:")
print(predictions.head(10))

"""# TASK 3: **Q14**"""

# Hierarchical (Horizontal) Clustering on the given dataset

import pandas as pd
import matplotlib.pyplot as plt
import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.preprocessing import StandardScaler

# Load dataset
df = pd.read_csv("shopping_behavior_100.csv")

# Use numeric fields
X_num = df[["Age", "Purchase Amount (USD)"]].dropna()


# Scale data
scaler = StandardScaler()
X = scaler.fit_transform(X_num)



# ---- Create clusters (choose k=4 as example) ----
agg = AgglomerativeClustering(n_clusters=4, linkage="ward")
labels = agg.fit_predict(X)


# ---- Scatter plot for Hierarchical clusters ----
plt.scatter(X[:, 0], X[:, 1], c=labels)
plt.title("Hierarchical (Agglomerative) Clusters")
plt.xlabel("Age (scaled)")
plt.ylabel("Purchase Amount (scaled)")
plt.show()

#Hierarchical (Agglomerative) Clustering
import pandas as pd # Import pandas
from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# 1. Load dataset
df = pd.read_csv("shopping_behavior_100.csv")


#Select numeric fields
cluster_data = df[['Age', 'Purchase Amount (USD)']].dropna()

linked = linkage(cluster_data, method = 'ward')
plt.figure(figsize=(8, 4))
dendrogram(linked, orientation = 'top', distance_sort = 'descending', show_leaf_counts = True)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Samples")
plt.ylabel("Distance")
plt.show()

"""# **TASK 3: Q14: Peform K-means for fields from the dataset**"""

k = 4
kmeans = KMeans(n_clusters=k, init="k-means++", random_state=42, n_init=10, max_iter=300)
labels = kmeans.fit_predict(X)

clustered = data.copy()
clustered["Cluster"] = labels

plt.figure()
for c in sorted(clustered["Cluster"].unique()):
    plt.scatter(
        clustered.loc[clustered["Cluster"] == c, "Age"],
        clustered.loc[clustered["Cluster"] == c, "Purchase Amount (USD)"],
        s=60,
        label=f"Cluster {c+1}"
    )



centroids_unscaled = pd.DataFrame(
    scaler.inverse_transform(kmeans.cluster_centers_),
    columns=feature_cols
)

plt.scatter(
    centroids_unscaled[feature_cols[0]],
    centroids_unscaled[feature_cols[1]],
    s=600, c="yellow", edgecolors="black", linewidths=1.2,
    label="Centroids", zorder=5
)

plt.title("K-Means Clustering", fontsize=18)
plt.xlabel("X")  # to mimic the example's axis labels
plt.ylabel("Y")
plt.legend(framealpha=0.9, title=None)
plt.tight_layout()
plt.show()


print("Cluster centroids (original units):")
print(centroids_unscaled)

# Using the Elbow method to find the optimal number of clusters:
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

df = pd.read_csv("shopping_behavior_100.csv")


feature_cols = ["Age", "Purchase Amount (USD)"]
data = df[feature_cols].dropna().reset_index(drop=True)

# 3) Scale features for K-Means
scaler = StandardScaler()
X = scaler.fit_transform(data)


wcss = []
for k in range(1, 11):
    km = KMeans(n_clusters=k, init="k-means++", random_state=42, n_init=10, max_iter=300)
    km.fit(X)
    wcss.append(km.inertia_)

plt.figure()
plt.plot(range(1, 11), wcss)
plt.title("Elbow Method (Age + Purchase Amount)")
plt.xlabel("Number of clusters (k)")
plt.ylabel("WCSS")
plt.show()